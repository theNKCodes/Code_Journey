{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3)\n",
    "clf_entropy.fit(X_train, Y_train)\n",
    "plt.figure(figsize=(20,15))\n",
    "tree.plot_tree(clf_entropy, feature_names=data.iloc[0], filled=True)\\\n",
    "y_pred = clf_object.predict(X_test)\n",
    "y_pred_entropy = prediction(X_test, clf_entropy)\n",
    "\n",
    "#KNN\n",
    "X_train, X_test = standardscaler(X_train, X_test)\n",
    "k_values = list(range(1, 20))\n",
    "scores = []\n",
    "for k in k_values:\n",
    "   knn = KNeighborsClassifier(n_neighbors=k)\n",
    "   score = cross_val_score(knn, X, Y, cv=3)\n",
    "   scores.append(np.mean(score))\n",
    "best_k = k_values[np.argmax(scores)]\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn.fit(X_train, Y_train)\n",
    "y_pred = best_knn.predict(X_test)\n",
    "sb.lineplot(x=k_values, y=scores, marker='o')\n",
    "\n",
    "#NB\n",
    "NBclassifier = GaussianNB()\n",
    "NBmodel = NBclassifier.fit(train, train_labels)\n",
    "NBpreds = NBclassifier.predict(test)\n",
    "print(\"PREDICTIONS ARE AS FOLLOWS :\\n\", NBpreds[:30])\n",
    "print(\"\\n ACCURACY : \", accuracy_score(test_labels, NBpred)*100)\n",
    "\n",
    "#KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import uniform\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "import random\n",
    "def euclidean(point, data):\n",
    "    \"\"\"\n",
    "    Euclidean distance between point & data.\n",
    "    Point has dimensions (m,), data has dimensions (n,m), and output will be of size (n,).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((point - data)**2, axis=1))\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=8, max_iter=300):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "    def fit(self, X_train):\n",
    "        # Initialize the centroids, using the \"k-means++\" method, where a random datapoint is selected as the first,\n",
    "        # then the rest are initialized w/ probabilities proportional to their distances to the first\n",
    "        # Pick a random point from train data for first centroid\n",
    "        self.centroids = [random.choice(X_train)]\n",
    "        for _ in range(self.n_clusters-1):\n",
    "            # Calculate distances from points to the centroids\n",
    "            dists = np.sum([euclidean(centroid, X_train) for centroid in self.centroids], axis=0)\n",
    "            # Normalize the distances\n",
    "            dists /= np.sum(dists)\n",
    "            # Choose remaining points based on their distances\n",
    "            new_centroid_idx, = np.random.choice(range(len(X_train)), size=1, p=dists)\n",
    "            self.centroids += [X_train[new_centroid_idx]]\n",
    "        # This initial method of randomly selecting centroid starts is less effective\n",
    "        # min_, max_ = np.min(X_train, axis=0), np.max(X_train, axis=0)\n",
    "        # self.centroids = [uniform(min_, max_) for _ in range(self.n_clusters)]\n",
    "        # Iterate, adjusting centroids until converged or until passed max_iter\n",
    "        iteration = 0\n",
    "        prev_centroids = None\n",
    "        while np.not_equal(self.centroids, prev_centroids).any() and iteration < self.max_iter:\n",
    "            # Sort each datapoint, assigning to nearest centroid\n",
    "            sorted_points = [[] for _ in range(self.n_clusters)]\n",
    "            for x in X_train:\n",
    "                dists = euclidean(x, self.centroids)\n",
    "                centroid_idx = np.argmin(dists)\n",
    "                sorted_points[centroid_idx].append(x)\n",
    "            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n",
    "            prev_centroids = self.centroids\n",
    "            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                if np.isnan(centroid).any():  # Catch any np.nans, resulting from a centroid having no points\n",
    "                    self.centroids[i] = prev_centroids[i]\n",
    "            iteration += 1\n",
    "    def evaluate(self, X):\n",
    "        centroids = []\n",
    "        centroid_idxs = []\n",
    "        for x in X:\n",
    "            dists = euclidean(x, self.centroids)\n",
    "            centroid_idx = np.argmin(dists)\n",
    "            centroids.append(self.centroids[centroid_idx])\n",
    "            centroid_idxs.append(centroid_idx)\n",
    "        return centroids, centroid_idxs\n",
    "# Create a dataset of 2D distributions\n",
    "centers = 5\n",
    "X_train, true_labels = make_blobs(n_samples=100, centers=centers, random_state=42)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "# Fit centroids to dataset\n",
    "kmeans = KMeans(n_clusters=centers)\n",
    "kmeans.fit(X_train)\n",
    "# View results\n",
    "class_centers, classification = kmeans.evaluate(X_train)\n",
    "sns.scatterplot(x=[X[0] for X in X_train],\n",
    "                y=[X[1] for X in X_train],\n",
    "                hue=true_labels,\n",
    "                style=classification,\n",
    "                palette=\"deep\",\n",
    "                legend=None\n",
    "                )\n",
    "plt.plot([x for x, _ in kmeans.centroids],\n",
    "         [y for _, y in kmeans.centroids],\n",
    "         'k+',\n",
    "         markersize=10,\n",
    "         )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Reinforcement\n",
    "GRID_SIZE = 5\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (GRID_SIZE - 1, GRID_SIZE - 1)\n",
    "OBSTACLES = [(1, 1), (2, 2), (3, 3)]\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EPSILON = 0.1\n",
    "NUM_EPISODES = 1000\n",
    "q_values = np.zeros((GRID_SIZE, GRID_SIZE, 4))\n",
    "def choose_action(state):\n",
    " if np.random.rand() < EPSILON:\n",
    " return np.random.choice(4)  # Explore\n",
    " else:\n",
    " return np.argmax(q_values[state])\n",
    "def q_learning():\n",
    " for episode in range(NUM_EPISODES):\n",
    "        state = START_STATE\n",
    " \twhile state != GOAL_STATE:\n",
    "            action = choose_action(state)\n",
    "            next_state = update_state(state, action)\n",
    "            reward = get_reward(next_state)\n",
    "            q_values[state][action] = (1 - LEARNING_RATE) * q_values[state][action] + \\\n",
    "         \t\t\t\t\tLEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max\n",
    "            state = next_state\n",
    "def update_state(state, action):\n",
    "    x, y = state\n",
    "    if action == 0:  # Up\n",
    "        x = max(0, x - 1)\n",
    "    elif action == 1:  # Down\n",
    "        x = min(GRID_SIZE - 1, x + 1)\n",
    "    elif action == 2:  # Left\n",
    "        y = max(0, y - 1)\n",
    "    elif action == 3:  # Right\n",
    "        y = min(GRID_SIZE - 1, y + 1)\n",
    " return (x, y)\n",
    "def get_reward(state):\n",
    " \treturn -1 if state in OBSTACLES else 0 if state != GOAL_STATE else 1\n",
    "q_learning()\n",
    " # Visualize the learned policy\n",
    "def visualize_policy():\n",
    "    policy = np.argmax(q_values, axis=2)\n",
    "    plt.imshow(policy, cmap='cool', origin='upper')\n",
    "    plt.title('Learned Policy for Grid World Navigation')\n",
    "    plt.show()\n",
    " visualize_policy()\n",
    "\n",
    "#3 Tensorflow\n",
    "graph = tf.Graph()\n",
    " with graph.as_default():\n",
    "  tensor_a = tf.constant(3.0)\n",
    "  tensor_b = tf.constant(4.2)\n",
    "  result = tf.add(tensor_a, tensor_b)\n",
    " print(graph)\n",
    "\n",
    "#4 Regression\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "#4 Kmeans\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "y_kmeans = kmeans.predict(X_scaled)\n",
    "\n",
    "#4 KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_knn_pred = knn.predict(X_test_scaled)\n",
    "accuracy_knn = accuracy_score(y_test, y_knn_pred)*100\n",
    "conf_matrix = confusion_matrix(y_test, y_knn_pred)\n",
    "\n",
    "#5 Ensemble\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "#6 MLP\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    " \t\t\trics=['accuracy'])\n",
    "history=model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_data=(test_images, tes\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc * 100}%')\n",
    "training_results = history\n",
    "train_loss = training_results.history['loss']\n",
    "train_acc = training_results.history['accuracy']\n",
    "valid_loss = training_results.history['val_loss']\n",
    "valid_acc = training_results.history['val_accuracy']\n",
    "plt.plot(train_loss, label=\"Training Loss\", color=\"g\")\n",
    "plt.plot(valid_loss, label=\"Validation Loss\", color=\"b\")\n",
    "plt.plot(train_acc, label=\"Training Accuracy\", color=\"g\")\n",
    "plt.plot(valid_acc, label=\"Validation Accuracy\", color=\"b\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
